# -*- coding: utf-8 -*-
"""credit card risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z56ptNbAHVOqHSAxnmGm3r4G9u4PLwjh
"""

pip install optuna

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import optuna

from sklearn.preprocessing import MinMaxScaler, FunctionTransformer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Load Data
df = pd.read_csv("/content/CC GENERAL.csv")
df.dropna(inplace=True)

# Feature Engineering: Log Transform to Handle Skewness
log_transformer = FunctionTransformer(np.log1p, validate=True)
df_transformed = log_transformer.fit_transform(df.select_dtypes(include=[np.number]))

# Feature Scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df_transformed)

# PCA for Dimensionality Reduction
pca = PCA(n_components=0.95)
pca_data = pca.fit_transform(scaled_data)

# ðŸ“Œ Function to Evaluate Clustering Models
def evaluate_clustering(model, data):
    labels = model.fit_predict(data)
    silhouette = silhouette_score(data, labels)
    db_score = davies_bouldin_score(data, labels)
    return silhouette, db_score, labels

# ðŸ“Œ Optimize K-Means with Optuna
def optimize_kmeans(trial):
    k = trial.suggest_int("k", 2, 12)
    model = KMeans(n_clusters=k, random_state=42, n_init=10)
    score, _, _ = evaluate_clustering(model, pca_data)
    return score

study = optuna.create_study(direction="maximize")
study.optimize(optimize_kmeans, n_trials=10)
best_k = study.best_params["k"]

# ðŸ“Œ Train Clustering Models
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
sil_kmeans, db_kmeans, kmeans_labels = evaluate_clustering(kmeans, pca_data)

dbscan = DBSCAN(eps=0.3, min_samples=10)
sil_dbscan, db_dbscan, dbscan_labels = evaluate_clustering(dbscan, pca_data)

gmm = GaussianMixture(n_components=best_k, random_state=42)
gmm_labels = gmm.fit_predict(pca_data)
sil_gmm = silhouette_score(pca_data, gmm_labels)
db_gmm = davies_bouldin_score(pca_data, gmm_labels)

agglo = AgglomerativeClustering(n_clusters=best_k)
sil_agglo, db_agglo, agglo_labels = evaluate_clustering(agglo, pca_data)

# ðŸ“Œ Choose Best Model
results = pd.DataFrame({
    "Model": ["K-Means", "DBSCAN", "GMM", "Hierarchical"],
    "Silhouette Score": [sil_kmeans, sil_dbscan, sil_gmm, sil_agglo],
    "Davies-Bouldin Score": [db_kmeans, db_dbscan, db_gmm, db_agglo]
})

print(results.sort_values(by="Silhouette Score", ascending=False))

df["Cluster"] = kmeans_labels if sil_kmeans > max(sil_dbscan, sil_gmm, sil_agglo) else (
    gmm_labels if sil_gmm > max(sil_dbscan, sil_agglo) else agglo_labels
)

# ðŸ“Œ Anomaly Detection
iso_forest = IsolationForest(contamination=0.05, random_state=42)
df["Anomaly_IF"] = iso_forest.fit_predict(pca_data)

lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)
df["Anomaly_LOF"] = lof.fit_predict(pca_data)

# ðŸ“Œ Customer Persona Analysis
df["High-Value Customer"] = (df["Cluster"] == df["Cluster"].value_counts().idxmax()).astype(int)
df["Low-Risk Customer"] = (df["Anomaly_IF"] == 1) & (df["Anomaly_LOF"] == 1)
df["Risky Customer"] = (df["Anomaly_IF"] == -1) | (df["Anomaly_LOF"] == -1)

# ðŸ“Œ Visualize Cluster Distribution
plt.figure(figsize=(10, 6))
sns.countplot(x=df["Cluster"], palette="viridis")
plt.title("Customer Cluster Distribution")
plt.show()

# ðŸ“Œ PCA Cluster Visualization
plt.figure(figsize=(8, 6))
sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=df["Cluster"], palette="viridis", alpha=0.6)
plt.title("PCA Clustering Visualization")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# Visualize Cluster Separation with Pairplot
plt.figure(figsize=(12, 10))
sns.pairplot(pd.DataFrame(pca_data[:, :3]).assign(Cluster=df["Cluster"]), hue="Cluster", palette="viridis", diag_kind="kde") # Reduced to first 3 PCs for visualization
plt.suptitle("Pairplot of Clusters in Reduced Dimensionality Space", y=1.02)
plt.show()

import matplotlib.pyplot as plt
# ðŸ“Œ Visualize Anomaly Detection Results
plt.figure(figsize=(10, 6))
sns.countplot(x=df["Anomaly_IF"], palette="viridis")
plt.title("Anomaly Detection (Isolation Forest)")
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x=df["Anomaly_LOF"], palette="viridis")
plt.title("Anomaly Detection (Local Outlier Factor)")
plt.show()

# ðŸ“Œ Visualize Customer Personas
plt.figure(figsize=(10, 6))
sns.countplot(x=df["High-Value Customer"], palette="viridis")
plt.title("High-Value Customer Distribution")
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x=df["Low-Risk Customer"], palette="viridis")
plt.title("Low-Risk Customer Distribution")
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x=df["Risky Customer"], palette="viridis")
plt.title("Risky Customer Distribution")
plt.show()

# ðŸ“Œ Boxplots for different features across clusters (example with BALANCE and PURCHASES)
plt.figure(figsize=(12, 6))
sns.boxplot(x="Cluster", y="BALANCE", data=df)
plt.title("Balance Distribution Across Clusters")
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x="Cluster", y="PURCHASES", data=df)
plt.title("Purchases Distribution Across Clusters")
plt.show()

# ðŸ“Œ Violin Plots for more detailed distribution visualization (example with ONEOFF_PURCHASES)
plt.figure(figsize=(12, 6))
sns.violinplot(x="Cluster", y="ONEOFF_PURCHASES", data=df)
plt.title("One-off Purchases Distribution Across Clusters")
plt.show()

